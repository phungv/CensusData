{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xlrd\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##NOTES##\n",
    "# Step 1: Load Files from Census Data and extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Build LogReference Table\n",
    "sheet_names = ['AK', 'AL', 'AR', 'AZ', 'CA', 'CO', 'CT', 'DC', 'DE',\n",
    " 'FL', 'GA', 'HI', 'IA', 'ID', 'IL', 'IN', 'KS', 'KY', 'LA', 'MA', 'MD', 'ME',\n",
    " 'MI', 'MN', 'MO', 'MS', 'MT', 'NC', 'ND', 'NE', 'NH', 'NJ', 'NM', 'NV', 'NY',\n",
    " 'OH', 'OK', 'OR', 'PA', 'PR', 'RI', 'SC', 'SD', 'TN', 'TX', 'US',\n",
    " 'UT', 'VA', 'VT', 'WA', 'WI', 'WV', 'WY']\n",
    "dfs = pd.read_excel(r\"C:\\Census Data\\References\\5yearMiniGeo.xlsx\", sheet_name=sheet_names)\n",
    "LogRecRef = pd.concat((df.assign(source=sheet) for sheet, df in dfs.items()), ignore_index=True)\n",
    "LogRecRef.to_csv(\"LogRecReference.csv\", sep=',', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##NOTES##\n",
    "# Step 2: Process and combine all Census files (.txt) into individual CSV files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set to the file extension of \"to-be-merged\" files\n",
    "ext = \".txt\"\n",
    "#set to your working directory\n",
    "dir_path = 'C:\\\\Census Data\\\\data'\n",
    "#set to the name of your output file\n",
    "#results = 'ACSSF_2012_2016_Full.txt'\n",
    "\n",
    "\n",
    "\n",
    "#for number in fib:\n",
    "    \n",
    "#fib = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,\n",
    "#32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,\n",
    "#62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,\n",
    "#93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,\n",
    "#118,119,120,121,122]\n",
    "\n",
    "\n",
    "\n",
    "for (i) in range(36,123):\n",
    "    \n",
    "    seqcount = \"000\"+str(i)         \n",
    "    lseqcount = seqcount[::-1][:4][::-1] + '000'\n",
    "    #print(lseqcount)\n",
    "    \n",
    "    Seqfile = r\"C:\\Census Data\\References\\2016_5yr_Summary_FileTemplates\\templates\\Seq\" + str(i) + \".xls\"    \n",
    "    #print(Seqfile)    \n",
    "    \n",
    "    CSVfile = r\"csvfile\" + str(i) + \".csv\"\n",
    "    #print(CSVfile)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    files = os.listdir('C:\\\\Census Data\\\\data')\n",
    "\n",
    "        #e20161ak0001000\n",
    "        #e = estimates\n",
    "        #2016 = year\n",
    "        #1 = 1 year estimate vs. 5 year\n",
    "        #ak = state\n",
    "        #0001 = sequence number\n",
    "        #000 = iterationID\n",
    "\n",
    "    files2 = [k for k in files if 'e' in k] \n",
    "    files3 = [k for k in files2 if lseqcount in k] \n",
    "    #files3b = [k for k in files3 if 'ma' in k]\n",
    "    files3a = [k for k in files3 if 'e20165' in k]\n",
    "\n",
    "\n",
    "    string = 'C:\\\\Census Data\\\\data\\\\'\n",
    "    files4 = [string + x for x in files3a]\n",
    "    files4\n",
    "\n",
    "\n",
    "\n",
    "    my_cols = [\"1\",\"YEAR\",\"STATE\",\"4\",\"SEQ\",\"LOGRECNO\",\"7\",\"8\",\"9\",\"10\",\"11\",\"12\",\"13\",\"14\",\"15\",\"16\",\"17\",\"18\",\"19\",\"20\",\"21\",\"22\",\"23\",\"24\",\"25\",\n",
    "    \"26\",\"27\",\"28\",\"29\",\"30\",\"31\",\"32\",\"33\",\"34\",\"35\",\"36\",\"37\",\"38\",\"39\",\"40\",\"41\",\"42\",\"43\",\"44\",\"45\",\"46\",\"47\",\"48\",\n",
    "    \"49\",\"50\",\"51\",\"52\",\"53\",\"54\",\"55\",\"56\",\"57\",\"58\",\"59\",\"60\",\"61\",\"62\",\"63\",\"64\",\"65\",\"66\",\"67\",\"68\",\"69\",\"70\",\"71\",\"72\",\n",
    "    \"73\",\"74\",\"75\",\"76\",\"77\",\"78\",\"79\",\"80\",\"81\",\"82\",\"83\",\"84\",\"85\",\"86\",\"87\",\"88\",\"89\",\"90\",\"91\",\"92\",\"93\",\"94\",\"95\",\"96\",\n",
    "    \"97\",\"98\",\"99\",\"100\",\"101\",\"102\",\"103\",\"104\",\"105\",\"106\",\"107\",\"108\",\"109\",\"110\",\"111\",\"112\",\"113\",\"114\",\"115\",\"116\",\n",
    "    \"117\",\"118\",\"119\",\"120\",\"121\",\"122\",\"123\",\"124\",\"125\",\"126\",\"127\",\"128\",\"129\",\"130\",\"131\",\"132\",\"133\",\"134\",\"135\",\"136\",\n",
    "    \"137\",\"138\",\"139\",\"140\",\"141\",\"142\",\"143\",\"144\",\"145\",\"146\",\"147\",\"148\",\"149\",\"150\",\"151\",\"152\",\"153\",\"154\",\"155\",\"156\",\n",
    "    \"157\",\"158\",\"159\",\"160\",\"161\",\"162\",\"163\",\"164\",\"165\",\"166\",\"167\",\"168\",\"169\",\"170\",\"171\",\"172\",\"173\",\"174\",\"175\",\"176\",\n",
    "    \"177\",\"178\",\"179\",\"180\",\"181\",\"182\",\"183\",\"184\",\"185\",\"186\",\"187\",\"188\",\"189\",\"190\",\"191\",\"192\",\"193\",\"194\",\"195\",\"196\",\n",
    "    \"197\",\"198\",\"199\",\"200\",\"201\",\"202\",\"203\",\"204\",\"205\",\"206\",\"207\",\"208\",\"209\",\"210\",\"211\",\"212\",\"213\",\"214\",\n",
    "    \"215\",\"216\",\"217\",\"218\",\"219\",\"220\",\"221\",\"222\",\"223\",\"224\",\"225\",\"226\",\"227\",\"228\",\"229\",\"230\",\"231\",\"232\",\"233\",\"234\",\n",
    "    \"235\",\"236\",\"237\",\"238\",\"239\",\"240\",\"241\",\"242\",\"243\",\"244\",\"245\",\"246\",\"247\",\"248\",\"249\",\"250\",\"251\",\"252\",\"253\",\n",
    "    \"254\",\"255\",\"256\",\"257\",\"258\",\"259\",\"260\",\"261\",\"262\",\"263\",\"264\",\"265\",\"266\",\"267\",\"268\",\"269\",\"270\",\"271\",\"272\",\n",
    "    \"273\",\"274\",\"275\",\"276\",\"277\",\"278\",\"279\",\"280\",\"281\",\"282\",\"283\",\"284\",\"285\",\"286\",\"287\",\"288\",\"289\",\"290\",\"291\",\n",
    "    \"292\",\"293\",\"294\",\"295\",\"296\",\"297\",\"298\",\"299\",\"300\",\"301\",\"302\",\"303\",\"304\",\"305\",\"306\",\"307\",\"308\",\"309\",\"310\",\"311\",\n",
    "    \"312\",\"313\",\"314\",\"315\",\"316\",\"317\",\"318\",\"319\",\"320\",\"321\",\"322\",\"323\",\"324\",\"325\",\"326\",\"327\",\"328\",\"329\",\"330\",\n",
    "    \"331\",\"332\",\"333\",\"334\",\"335\",\"336\",\"337\",\"338\",\"339\",\"340\",\"341\",\"342\",\"343\",\"344\",\"345\",\"346\",\"347\",\"348\",\"349\",\"350\",\n",
    "    \"351\",\"352\",\"353\",\"354\",\"355\",\"356\",\"357\",\"358\",\"359\",\"360\",\"361\",\"362\",\"363\",\"364\",\"365\",\"366\",\"367\",\"368\",\"369\",\"370\",\n",
    "    \"371\",\"372\",\"373\",\"374\",\"375\",\"376\",\"377\",\"378\",\"379\",\"380\",\"381\",\"382\",\"383\",\"384\",\"385\",\"386\",\"387\",\"388\",\"389\",\"390\",\n",
    "    \"391\",\"392\",\"393\",\"394\",\"395\",\"396\",\"397\",\"398\",\"399\",\"400\",\"401\",\"402\",\"403\",\"404\",\"405\",\"406\",\"407\",\"408\",\"409\",\n",
    "    \"410\",\"411\",\"412\",\"413\",\"414\",\"415\",\"416\",\"417\",\"418\",\"419\",\"420\",\"421\",\"422\",\"423\",\"424\",\"425\",\"426\",\"427\",\"428\",\"429\",\n",
    "    \"430\",\"431\",\"432\",\"433\",\"434\",\"435\",\"436\",\"437\",\"438\",\"439\",\"440\",\"441\",\"442\",\"443\",\"444\",\"445\",\"446\",\"447\",\"448\",\n",
    "    \"449\",\"450\",\"451\",\"452\",\"453\",\"454\",\"455\",\"456\",\"457\",\"458\",\"459\",\"460\",\"461\",\"462\",\"463\",\"464\",\"465\",\"466\",\"467\",\n",
    "    \"468\",\"469\",\"470\",\"471\",\"472\",\"473\",\"474\",\"475\",\"476\",\"477\",\"478\",\"479\",\"480\",\"481\",\"482\",\"483\",\"484\",\"485\",\"486\",\n",
    "    \"487\",\"488\",\"489\",\"490\",\"491\",\"492\",\"493\",\"494\",\"495\",\"496\",\"497\",\"498\",\"499\",\"500\",\"501\",\"502\",\"503\",\"504\",\"505\",\n",
    "    \"506\",\"507\",\"508\",\"509\",\"510\",\"511\",\"512\",\"513\",\"514\",\"515\",\"516\",\"517\",\"518\",\"519\",\"520\",\"521\",\"522\",\"523\",\"524\",\n",
    "    \"525\",\"526\",\"527\",\"528\",\"529\",\"530\",\"531\",\"532\",\"533\",\"534\",\"535\",\"536\",\"537\",\"538\",\"539\",\"540\",\"541\",\"542\",\"543\",\n",
    "    \"544\",\"545\",\"546\",\"547\",\"548\",\"549\",\"550\",\"551\",\"552\",\"553\",\"554\",\"555\",\"556\",\"557\"]\n",
    "\n",
    "    fullfile = pd.read_csv('C:\\\\Census Data\\\\data\\\\e20165ak0001000.txt', names=my_cols, engine='python')\n",
    "    fullfile = fullfile.loc[fullfile['557'] == 0]\n",
    "    #fullfile\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    for f in files4:\n",
    "      if f.endswith(ext):\n",
    "        data = open(f)\n",
    "\n",
    "        #out = pd.read_csv(data, names=my_cols, engine='python')\n",
    "        out = pd.read_csv(data, names=my_cols, engine='python')\n",
    "\n",
    "        out2 = out\n",
    "\n",
    "\n",
    "        fullfile = pd.concat([fullfile, out2], sort=False, ignore_index=True)\n",
    "        data.close()\n",
    "\n",
    "\n",
    "\n",
    "    fullfile[\"STATE\"] = fullfile['STATE'].str.upper() #uppercase space\n",
    "    fullfile3 = fullfile.dropna(axis=1, how='all') #, thresh=None, subset=None, inplace=False)\n",
    "    fullfile2 = fullfile3\n",
    "\n",
    "    #fullfile2.to_csv(\"techers.csv\", sep=',', encoding='utf-8')\n",
    "    #out\n",
    "    #fullfile\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    #Macro1\n",
    "    #columnnames = pd.read_excel(r\"C:\\Census Data\\References\\2016_5yr_Summary_FileTemplates\\templates\\Seq41.xls\", sheet_name='E')\n",
    "    columnnames = pd.read_excel(Seqfile, sheet_name='E')\n",
    "    values = {'FILEID': \"1\", 'FILETYPE': \"YEAR\", 'STUSAB': \"STATE\", \n",
    "                                    'CHARITER': \"4\",\"SEQUENCE\" : \"SEQ\", \"LOGRECNO\": \"LOGRECNO\"\n",
    "              }\n",
    "\n",
    "    columnnames2 = columnnames.fillna(value = values)\n",
    "    #columnnames2 = columnnames2.rename_axis('COLNAMES')\n",
    "\n",
    "    name_str = columnnames2.transpose() \n",
    "    \n",
    "    name_str = name_str.drop([0], 1)\n",
    "    name_str['index1'] = name_str.index\n",
    "    name_str = name_str['index1'].replace({'STUSAB': 'STATE'})\n",
    " \n",
    "\n",
    "    print(\"Seqfile = \" + Seqfile)    \n",
    "    print(\"lseqcount = \" + lseqcount)\n",
    "    print(\"CSVfile = \" + str(CSVfile))\n",
    "    print(\"fullfile = \" + str(len(fullfile.columns)) )\n",
    "    print(\"fullfile3 = \" + str(len(fullfile3.columns)) )\n",
    "    print(\"name_str = \" + str(name_str.count())  )\n",
    "    \n",
    "\n",
    "    fullfile2.columns = name_str.values\n",
    "    fullfile2.columns = fullfile2.columns.map(str)\n",
    "    fullfile2.columns = fullfile2.columns.str.strip().str.replace(\":\", '').str.replace(\"'\", '').str.replace(',', '').str.replace('(', '').str.replace(')', '')\n",
    "\n",
    "    geofile = pd.merge(fullfile2, LogRecRef, on = ['LOGRECNO', 'STATE'], how = \"left\")\n",
    "    geofile = geofile.drop(['source'], 1)\n",
    "\n",
    "\n",
    "    geofile.to_csv(CSVfile, sep=',', encoding='utf-8')\n",
    "    #macro2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##NOTES##\n",
    "# Step 3: Combine all CSVs into large Full File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C:\\\\Census Data - Uncleaned Headers\\\\Cleaned CSVs\\\\BK',\n",
       " 'C:\\\\Census Data - Uncleaned Headers\\\\Cleaned CSVs\\\\csvfile41.csv',\n",
       " 'C:\\\\Census Data - Uncleaned Headers\\\\Cleaned CSVs\\\\csvfile42.csv',\n",
       " 'C:\\\\Census Data - Uncleaned Headers\\\\Cleaned CSVs\\\\csvfile43.csv',\n",
       " 'C:\\\\Census Data - Uncleaned Headers\\\\Cleaned CSVs\\\\csvfile48.csv',\n",
       " 'C:\\\\Census Data - Uncleaned Headers\\\\Cleaned CSVs\\\\csvfile59.csv',\n",
       " 'C:\\\\Census Data - Uncleaned Headers\\\\Cleaned CSVs\\\\csvfile63.csv',\n",
       " 'C:\\\\Census Data - Uncleaned Headers\\\\Cleaned CSVs\\\\csvfile64.csv',\n",
       " 'C:\\\\Census Data - Uncleaned Headers\\\\Cleaned CSVs\\\\csvfile78.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set to the file extension of \"to-be-merged\" files\n",
    "ext = \".csv\"\n",
    "#set to your working directory\n",
    "dir_path = 'C:\\\\Census Data - Uncleaned Headers\\\\Cleaned CSVs'\n",
    "#set to the name of your output file\n",
    "#results = 'ACSSF_2012_2016_Full.txt'\n",
    "\n",
    "files = os.listdir('C:\\\\Census Data - Uncleaned Headers\\\\Cleaned CSVs')\n",
    "\n",
    "#e20161ak0001000\n",
    "#e = estimates\n",
    "#2016 = year\n",
    "#1 = 1 year estimate vs. 5 year\n",
    "#ak = state\n",
    "#0001 = sequence number\n",
    "#000 = iterationID\n",
    "\n",
    "#files2 = [k for k in files if '11' in k]\n",
    "#files3 = [k for k in files2 if 'e2016' in k]\n",
    "#files3 = [k for k in files2 if '0041' in k]\n",
    "\n",
    "\n",
    "string = 'C:\\\\Census Data - Uncleaned Headers\\\\Cleaned CSVs\\\\'\n",
    "files4 = [string + x for x in files]\n",
    "files4 \n",
    "\n",
    "#File for Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfile = pd.read_csv(\"C:\\\\Census Data - Uncleaned Headers\\\\LogRecReference.csv\", engine='python')\n",
    "logfile = logfile.drop(['source','Unnamed: 0','Name'], 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in files4:\n",
    "  if f.endswith(ext):\n",
    "    data = open(f)\n",
    "    \n",
    "        \n",
    "    inputcsv = pd.read_csv(data,  engine='python')\n",
    "    \n",
    "    inputcsv = inputcsv.drop(['Unnamed: 0','GEOID','Name','FILETYPE','FILEID','CHARITER','SEQUENCE'], 1)\n",
    "    #inputcsv = inputcsv.drop(['4','1','Unnamed: 0','GEOID','Name','YEAR'], 1)\n",
    "    #inputcsv = inputcsv.rename(index=str, columns= {'SEQUENCE' : 'SEQ'})\n",
    "\n",
    "    \n",
    "    #newcolumn = inputcsv['SEQ'].values\n",
    "    #seq_num = str(newcolumn[0])[0:2]\n",
    "    #new_Seq_columnname = \"SEQ\" + seq_num + \"_\"   \n",
    "\n",
    "    #columnnames = inputcsv.columns[~inputcsv.columns.str.contains('STATE|SEQ|LOGRECNO')]\n",
    "    #inputcsv.rename(columns = dict(zip(columnnames, new_Seq_columnname + columnnames)),inplace=True)\n",
    "    \n",
    "    logfile = pd.merge(logfile, inputcsv, on = ['LOGRECNO', 'STATE'], how = \"left\")    \n",
    "    \n",
    "    #newcolumn = logfile['SEQ'].values\n",
    "\n",
    "    #seq_num = str(newcolumn[0])[0:2]\n",
    "    #seq_num = seq_num[0: seq_num.find(\".\")]\n",
    "    #new_Seq_columnname = \"SEQ_\"+ seq_num\n",
    "        \n",
    "\n",
    "    #logfile.rename(columns = {'SEQ' : new_Seq_columnname}, inplace=True)\n",
    "    \n",
    "    data.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "logfile.to_csv(\"\\\\\\\\Aopscifs004.amsa.com\\\\fs$\\\\DATA\\\\GROUP\\\\Insights and Outcomes\\\\IOG Projects\\\\National School District View\\\\Data\\\\Census Data\\\\5 Year Census Data\\\\2011-2016\\\\CB2011_2016_Selected.csv\", sep=',', encoding='utf-8') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
